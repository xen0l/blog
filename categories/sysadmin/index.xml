<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sysadmin on xenol&#39;s blog</title>
    <link>https://blog.xenol.eu/categories/sysadmin/</link>
    <description>Recent content in Sysadmin on xenol&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 03 Sep 2016 12:10:21 +0200</lastBuildDate>
    <atom:link href="https://blog.xenol.eu/categories/sysadmin/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Managing illumos networking with Ansible</title>
      <link>https://blog.xenol.eu/blog/2016/09/03/Managing-illumos-networking-with-Ansible/</link>
      <pubDate>Sat, 03 Sep 2016 12:10:21 +0200</pubDate>
      
      <guid>https://blog.xenol.eu/blog/2016/09/03/Managing-illumos-networking-with-Ansible/</guid>
      <description>&lt;p&gt;On August 30th, my work on Ansible modules for managing Solaris/illumos networking was merged into &lt;a href=&#34;https://github.com/ansible/ansible-modules-extras/commits/devel&#34;&gt;ansible-modules-extras&lt;/a&gt; repository. This functionality will be available in the next release of Ansible. Following modules were contributed:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.ansible.com/ansible/dladm_etherstub_module.html&#34;&gt;dladm_etherstub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.ansible.com/ansible/dladm_vnic_module.html&#34;&gt;dladm_vnic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.ansible.com/ansible/ipadm_if_module.html&#34;&gt;ipadm_if&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.ansible.com/ansible/ipadm_prop_module.html&#34;&gt;ipadm_prop&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.ansible.com/ansible/flowadm_module.html&#34;&gt;flowadm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This also marks a major milestone for automating the deployment of Solaris/illumos zones with Ansible. There already was a module for creating/deleting zones and manipulating their state - &lt;a href=&#34;https://docs.ansible.com/ansible/solaris_zone_module.html&#34;&gt;solaris_zone&lt;/a&gt;. However, there was not a way to manage the networking aspect of zones until now.&lt;/p&gt;

&lt;p&gt;In the future, I have plans to contribute more modules for Solaris/illumos networking management and also other aspects of these systems. I have collected some module ideas in the &lt;a href=&#34;https://github.com/xen0l/ansible-illumos-modules&#34;&gt;Github repository&lt;/a&gt;. If you have any module ideas, be sure to create an issue of send a PR to the repository.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mirroring OpenIndiana IPS repositories - part 1</title>
      <link>https://blog.xenol.eu/blog/2016/08/09/Mirroring-OpenIndiana-IPS-repositories---part-1/</link>
      <pubDate>Tue, 09 Aug 2016 00:09:28 +0200</pubDate>
      
      <guid>https://blog.xenol.eu/blog/2016/08/09/Mirroring-OpenIndiana-IPS-repositories---part-1/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;In this blog post, I will show how one can create a mirror of OpenIndiana IPS
repositories. IPS repositories hold packages coming from multiple sources, most
notably from &lt;a href=&#34;https://github.com/illumos/illumos-gate&#34;&gt;illumos-gate&lt;/a&gt; and
&lt;a href=&#34;https://github.com/OpenIndiana/oi-userland&#34;&gt;oi-userland&lt;/a&gt;. Every time user runs
&lt;strong&gt;pkg update&lt;/strong&gt; on his OpenIndiana installation, &lt;strong&gt;pkg&lt;/strong&gt;  contacts IPS repository
to fetch catalog metadata and determines if there are any new updates available.
If updates are available, &lt;strong&gt;pkg&lt;/strong&gt; downoalds only changed files over HTTP.
IPS application server is written in Python and &lt;a href=&#34;http://www.cherrypy.org/&#34;&gt;CherryPy&lt;/a&gt;
web framework. I will also describe how to configure nginx as
a reverse proxy for IPS mirror and make it more robust. The nice thing about
setting IPS mirror is that it is very easy to do as we have it deeply integrated
with other illumos technologies: SMF and ZFS.&lt;/p&gt;

&lt;p&gt;If unsure whetever you should create your own IPS repository mirror, consider these
use cases:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;have a reasonable big OpenIndiana infrastructure and want to have a better control over updates&lt;/li&gt;
&lt;li&gt;save network bandwidth&lt;/li&gt;
&lt;li&gt;want to provide IPS repository mirror for the community&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is enough to do steps in the first part of this blog post to get IPS mirror up and running. This
configuration should also work on other illumos IPS distributions such as OmniOS with minimal changes.
In the next part, I will mainly concentrate on how to increase IPS server
performance by leveraging caching, nginx settings and deploying TLS.&lt;/p&gt;

&lt;h1 id=&#34;preparation&#34;&gt;Preparation&lt;/h1&gt;

&lt;p&gt;I will be using OpenIndiana hipster 2016.04 [vagrant]{&lt;a href=&#34;https://www.vagrantup.com/&#34;&gt;https://www.vagrantup.com/&lt;/a&gt;} box as my working
environemnt. It provides me with a basic setup: base install, network configuration and access to the
Internet. The user &lt;strong&gt;vagrant&lt;/strong&gt; has &lt;strong&gt;Primary Administrator&lt;/strong&gt; RBAC role assigned, so it can get access
to superuser privileges. Configuring these is out of scope of this document.&lt;/p&gt;

&lt;h1 id=&#34;configuring-ips-mirroring-service&#34;&gt;Configuring IPS mirroring service&lt;/h1&gt;

&lt;p&gt;IPS mirroring service is provided by &lt;strong&gt;svc:/application/pkg/mirror:default&lt;/strong&gt; SMF service. This service
will configure everything necesseary from creating ZFS datasets for every pkg/mirror instance to creating
crontab entry.&lt;/p&gt;

&lt;p&gt;We start by adding publishers, which we want to mirror from. I will mirror two publishers:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;openindiana.org&lt;/strong&gt; publisher from &lt;strong&gt;&lt;a href=&#34;http://pkg.openindiana.org/hipster&#34;&gt;http://pkg.openindiana.org/hipster&lt;/a&gt;&lt;/strong&gt; repository (&lt;strong&gt;~77GB&lt;/strong&gt; as of August 2016)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;hipster-encumbered&lt;/strong&gt; publisher from &lt;strong&gt;&lt;a href=&#34;http://pkg.openindiana.org/hipster-encumbered&#34;&gt;http://pkg.openindiana.org/hipster-encumbered&lt;/a&gt;&lt;/strong&gt; repository (&lt;strong&gt;~1GB&lt;/strong&gt; as of August 2016)&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;vagrant@openindiana:/export/home/vagrant$ pkg publisher
PUBLISHER                   TYPE     STATUS P LOCATION
openindiana.org             origin   online F http://pkg.openindiana.org/hipster/
vagrant@openindiana:/export/home/vagrant$ pfexec pkg set-publisher \
                                            -p http://pkg.openindiana.org/hipster-encumbered
pkg set-publisher:
  Added publisher(s): hipster-encumbered
vagrant@openindiana:/export/home/vagrant$ pkg publisher
PUBLISHER                   TYPE     STATUS P LOCATION
openindiana.org             origin   online F http://pkg.openindiana.org/hipster/
hipster-encumbered          origin   online F http://pkg.openindiana.org/hipster-encumbered/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Please note that &lt;strong&gt;openindiana.org&lt;/strong&gt; IPS publisher is preconfigured by default. If it is not present, add it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pfexec pkg set-publisher -p http://pkg.openindiana.org/hipster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We need to create &lt;strong&gt;rpool/VARSHARE/pkg&lt;/strong&gt; ZFS dataset as &lt;strong&gt;pkg/mirror&lt;/strong&gt; expects it. I placed it on a root pool as &lt;strong&gt;pkg/mirror&lt;/strong&gt; expects
&amp;lt;BE root pool&amp;gt;/VARSHARE. We also enable &lt;strong&gt;lz4 compression&lt;/strong&gt;
on this dataset, so  child datasets inherit the compression setting. Now, create the dataset:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;vagrant@openindiana:/export/home/vagrant$ pfexec zfs create -o mountpoint=/var/share rpool/VARSHARE
vagrant@openindiana:/export/home/vagrant$ pfexec zfs create -o compression=lz4 rpool/VARSHARE/pkg
vagrant@openindiana:/export/home/vagrant$ zfs list
NAME                         USED  AVAIL  REFER  MOUNTPOINT
rpool                       4.43G  43.8G  29.5K  /rpool
rpool/ROOT                  2.36G  43.8G    19K  legacy
rpool/ROOT/openindiana      2.36G  43.8G  2.18G  /
rpool/ROOT/openindiana/var   119M  43.8G   107M  /var
rpool/VARSHARE                38K  43.8G    19K  /var/share
rpool/VARSHARE/pkg            19K  43.8G    19K  /var/share/pkg
rpool/dump                  1.00G  43.8G  1.00G  -
rpool/export                65.5K  43.8G    19K  /export
rpool/export/home           46.5K  43.8G    19K  /export/home
rpool/export/home/vagrant   27.5K  43.8G  27.5K  /export/home/vagrant
rpool/swap                  1.06G  44.7G   117M  -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We continue with configuring &lt;strong&gt;pkg/mirror&lt;/strong&gt; instances for every IPS publisher:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;svccfg -s pkg/mirror add hipster
svccfg -s pkg/mirror:hipster addpg config application
svccfg -s pkg/mirror:hipster setprop config/cache_dir=&amp;quot;/var/cache/pkg/mirror&amp;quot;
svccfg -s pkg/mirror:hipster setprop config/crontab_period=&#39;&amp;quot;0 18,6 * * *&amp;quot;&#39;
svccfg -s pkg/mirror:hipster setprop config/publishers=&amp;quot;openindiana.org&amp;quot;
svccfg -s pkg/mirror:hipster setprop config/repository=&amp;quot;/var/share/pkg/repositories/hipster&amp;quot;

svccfg -s pkg/mirror add hipster-encumbered
svccfg -s pkg/mirror:hipster-encumbered addpg config application
svccfg -s pkg/mirror:hipster-encumbered setprop config/cache_dir=&amp;quot;/var/cache/pkg/mirror&amp;quot;
svccfg -s pkg/mirror:hipster-encumbered setprop config/crontab_period=&#39;&amp;quot;0 17,4 * * *&amp;quot;&#39;
svccfg -s pkg/mirror:hipster-encumbered setprop config/publishers=astring: &amp;quot;hipster-encumbered&amp;quot;
svccfg -s pkg/mirror:hipster-encumbered setprop config/repository=&amp;quot;/var/share/pkg/repositories/hipster-encumbered&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once SMF instances are created and configured, we need to start them and also a service called
&lt;strong&gt;svc:/application/pkg/repositories-setup:default&lt;/strong&gt;, which will create ZFS datasets:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;vagrant@openindiana:/export/home/vagrant$ svcs -a | grep pkg/mirror
disabled       23:34:02 svc:/application/pkg/mirror:default
-              -        svc:/application/pkg/mirror:hipster
-              -        svc:/application/pkg/mirror:hipster-encumbered
vagrant@openindiana:/export/home/vagrant$ svcadm enable svc:/application/pkg/mirror:hipster
vagrant@openindiana:/export/home/vagrant$ svcadm enable svc:/application/pkg/mirror:hipster-encumbered
vagrant@openindiana:/export/home/vagrant$ svcadm enable svc:/application/pkg/repositories-setup:default
vagrant@openindiana:/export/home/vagrant$ svcs -xv
vagrant@openindiana:/export/home/vagrant$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ZFS datasets for &lt;strong&gt;pkg/mirror&lt;/strong&gt; instances have been created and entries were added to &lt;strong&gt;pkg5serv&lt;/strong&gt;&amp;rsquo;s crontab:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;vagrant@openindiana:/export/home/vagrant$ zfs list
NAME                                                 USED  AVAIL  REFER  MOUNTPOINT
rpool                                               4.43G  43.8G  29.5K  /rpool
rpool/ROOT                                          2.36G  43.8G    19K  legacy
rpool/ROOT/openindiana                              2.36G  43.8G  2.18G  /
rpool/ROOT/openindiana/var                           119M  43.8G   107M  /var
rpool/VARSHARE                                        96K  43.8G    19K  /var/share
rpool/VARSHARE/pkg                                    77K  43.8G    19K  /var/share/pkg
rpool/VARSHARE/pkg/repositories                       58K  43.8G    19K  /var/share/pkg/repositories
rpool/VARSHARE/pkg/repositories/hipster             19.5K  43.8G  19.5K  /var/share/pkg/repositories/hipster
rpool/VARSHARE/pkg/repositories/hipster-encumbered  19.5K  43.8G  19.5K  /var/share/pkg/repositories/hipster-encumbered
rpool/dump                                          1.00G  43.8G  1.00G  -
rpool/export                                        65.5K  43.8G    19K  /export
rpool/export/home                                   46.5K  43.8G    19K  /export/home
rpool/export/home/vagrant                           27.5K  43.8G  27.5K  /export/home/vagrant
rpool/swap                                          1.06G  44.7G   117M  -
pkg5srv@openindiana:~$ /usr/bin/crontab -l
0 17,4 * * * /usr/sbin/svcadm refresh svc:/application/pkg/mirror:hipster-encumbered
0 18,6 * * * /usr/sbin/svcadm refresh svc:/application/pkg/mirror:hipster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The data will appear in &lt;strong&gt;/var/cache/pkg/repositories/hispter&lt;/strong&gt; and &lt;strong&gt;/var/cache/pkg/repositories/hispter-encumbered&lt;/strong&gt;
after cron runs initial synchronization. Please, keep in mind that initial sync may take longer depending on your network
connection speed.&lt;/p&gt;

&lt;h1 id=&#34;serving-the-data&#34;&gt;Serving the data&lt;/h1&gt;

&lt;p&gt;Once the data is mirrored, we need to serve it to users. This is accomplished by creating &lt;strong&gt;pkg.depot&lt;/strong&gt; instance for every
mirrored repository. The &lt;strong&gt;pkg.depot&lt;/strong&gt; comes installed with &lt;strong&gt;package/pkg&lt;/strong&gt; package and should be present on the system.
Creating IPS repository instance is just a matter of configuring a few SMF services (replace &amp;lt;DOMAIN&amp;gt; with a hostname you want to use,
I will use &lt;strong&gt;pkg.domain.tld&lt;/strong&gt; in this example):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;svccfg -s svc:/application/pkg/server add hipster
svccfg -s svc:/application/pkg/server:hipster addpg pkg application
svccfg -s svc:/application/pkg/server:hipster setprop \
                            pkg/inst_root=&amp;quot;/var/share/pkg/repositories/hipster&amp;quot;
svccfg -s svc:/application/pkg/server:hipster setprop \
                            pkg/port=&amp;quot;10100&amp;quot;
svccfg -s svc:/application/pkg/server:hipster setprop \
                            pkg/pkg_root=&amp;quot;/&amp;quot;
svccfg -s svc:/application/pkg/server:hipster setprop \
                            pkg/readonly=&amp;quot;true&amp;quot;
svccfg -s svc:/application/pkg/server:hipster setprop \
                            pkg/proxy_base=&amp;quot;http://&amp;lt;DOMAIN&amp;gt;/hipster&amp;quot;
svccfg -s svc:/application/pkg/server:hipster setprop \
                            pkg/address=&amp;quot;127.0.0.1&amp;quot;

svccfg -s svc:/application/pkg/server add hipster-encumbered
svccfg -s svc:/application/pkg/server:hipster-encumbered addpg pkg application
svccfg -s svc:/application/pkg/server:hipster-encumbered setprop \
                            pkg/inst_root=&amp;quot;/var/share/pkg/repositories/hipster-encumbered&amp;quot;
svccfg -s svc:/application/pkg/server:hipster-encumbered setprop \
                            pkg/port=&amp;quot;10101&amp;quot;
svccfg -s svc:/application/pkg/server:hipster-encumbered setprop \
                            pkg/pkg_root=&amp;quot;/&amp;quot;
svccfg -s svc:/application/pkg/server:hipster-encumbered setprop \
                            pkg/readonly=&amp;quot;true&amp;quot;
svccfg -s svc:/application/pkg/server:hipster-encumbered setprop \
                            pkg/proxy_base=&amp;quot;http://&amp;lt;DOMAIN&amp;gt;/hipster-encumbered&amp;quot;
svccfg -s svc:/application/pkg/server:hipster-encumbered setprop \
                            pkg/address=&amp;quot;127.0.0.1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After &lt;strong&gt;pkg/server&lt;/strong&gt; instances have been configured, ensure thay are present and start them:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;vagrant@openindiana:/export/home/vagrant$ svcs pkg/server
STATE          STIME    FMRI
disabled       Aug_05   svc:/application/pkg/server:default
-              -        svc:/application/pkg/server:hipster
-              -        svc:/application/pkg/server:hipster-encumbered
vagrant@openindiana:/export/home/vagrant$ pfexec svcadm enable pkg/server:hipster
vagrant@openindiana:/export/home/vagrant$ pfexec svcadm enable pkg/server:hipster-encumbered
vagrant@openindiana:/export/home/vagrant$ svcs -xv
vagrant@openindiana:/export/home/vagrant$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If everything went good, two &lt;strong&gt;pkg.depotd&lt;/strong&gt; instances should be running:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;vagrant@openindiana:/export/home/vagrant$ svcs -p pkg/server
STATE          STIME    FMRI
disabled       Aug_05   svc:/application/pkg/server:default
online         13:37:02 svc:/application/pkg/server:hipster
               13:37:02     2107 pkg.depotd
online         13:37:04 svc:/application/pkg/server:hipster-encumbered
               13:37:04     2131 pkg.depotd
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;configuring-nginx-as-a-reverse-proxy&#34;&gt;Configuring nginx as a reverse proxy&lt;/h1&gt;

&lt;p&gt;Using reverse proxy with &lt;strong&gt;pkg.depot&lt;/strong&gt; is essential as we can increase performance and security by proper configuration. Many reverse proxy implementations also come with caching and loadbalancing features, which we
can use as well. Another important thing is to configure a webserver for serving static files. I chose &lt;a href=&#34;https://www.nginx.org&#34;&gt;nginx&lt;/a&gt; as it excels at serving static files, reverse proxying and caching. Install the package:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pkg install nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once the package is installed, copy the following configuration snippet into &lt;strong&gt;/etc/nginx/nginx.conf&lt;/strong&gt; (or other place where nginx can find it), so nginx can start handling requests arriving at our IPS mirror instances from the outside world. Do not forget to replace &lt;strong&gt;pkg.domain.tld&lt;/strong&gt; with the actual hostname for the newly configured IPS mirror:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;server {
  listen 80;
  server_name pkg.domain.tld;

  access_log /var/nginx/logs/pkg.domain.tld-access.log main;
  error_log /var/nginx/logs/pkg.domain.tld-error.log debug;

  root /var/share/pkg/repositories;

  location /hipster/ {

    location ~ ^/hipster/file/0/(..)(.*)$ {
      try_files /hipster/publisher/openindiana.org/file/$1/$1$2 =404;
    }

    location ~ ^/hipster/openindiana.org/file/1/(..)(.*)$ {
      try_files /hipster/publisher/openindiana.org/file/$1/$1$2 =404;
    }

    location ~^/hipster/(open|abandon|add|close)/ {
      allow 127.0.0.0/8;
      deny all;
    }

    rewrite ^/hipster/$ / break;
    rewrite ^/hipster/(.*) /$1 break;

    proxy_pass http://127.0.0.1:10100;
    proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;
    proxy_redirect off;
    proxy_buffering off;
  }

  location /hipster-encumbered/ {

    location ~ ^/hipster-encumbered/file/0/(..)(.*)$ {
      try_files /hipster-encumbered/publisher/hipster-encumbered/file/$1/$1$2 =404;
    }

    location ~ ^/hipster-encumbered/hipster-encumbered/file/1/(..)(.*)$ {
      try_files /hipster-encumbered/publisher/hipster-encumbered/file/$1/$1$2 =404;
    }

    location ~^/hipster-encumbered/(open|abandon|add|close)/ {
      allow 127.0.0.0/8;
      deny all;
    }

    rewrite ^/hipster-encumbered/$ / break;
    rewrite ^/hipster-encumbered/(.*) /$1 break;

    proxy_pass http://127.0.0.1:10101;
    proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;
    proxy_redirect off;
    proxy_buffering off;
  }

  location / {
    return 301 http://pkg.domain.tld/hipster/en/index.shtml;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Before enabling nginx service, test if the configuration is correct and don&amp;rsquo;t forget to check for
service issues afterwards:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;vagrant@openindiana:/export/home/vagrant$ pfexec nginx -t
nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf test is successful
vagrant@openindiana:/export/home/vagrant$ svcadm enable nginx
vagrant@openindiana:/export/home/vagrant$ svcs -xv
vagrant@openindiana:/export/home/vagrant$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We are now ready to serve IPS data to the outside world. In the next chapter, we will look at how
to get maximum out of nginx capabilities.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DNSSEC automatization with OpenDNSSEC</title>
      <link>https://blog.xenol.eu/2012/10/29/dnssec-automatization-with-opendnssec/</link>
      <pubDate>Mon, 29 Oct 2012 00:00:00 +0000</pubDate>
      
      <guid>https://blog.xenol.eu/2012/10/29/dnssec-automatization-with-opendnssec/</guid>
      <description>&lt;p&gt;DNSSEC is an amazing piece of technology. DNSSEC data is digitally signed. The validating DNS server can check if the data it receives is identical to those on the authoritative DNS server. This helps us mitigate DNS cache poisoning.&lt;/p&gt;

&lt;p&gt;I have signed my domain back in January 2012, signing my zone by hand. However, I forgot to resign my zone and the zone signature expired making it unresolvable. This made me wonder how could I automatize the whole process. I read about &lt;a href=&#34;https://www.opendnssec.org&#34;&gt;OpenDNSSEC&lt;/a&gt;. OpenDNSSEC is a wonderful piece of software, which automates the DNSSEC zone managements. OpenDNSSEC is used by several NICs around the world to manage their TLD zones. I didn&amp;rsquo;t try OpenDNSSEC at first as I found it to be too complex and not suitable for a single zone. I tried BIND 9.9 DNSSEC inline-signing, instead. It worked, but I was unhappy with it. Inline signing in BIND converts manually maintained zone into a dynamic one and signs it. All DNSSEC changes are made to the journal file format, which I dislike working with. As I wasn&amp;rsquo;t satisfied with this solution, I gave OpenDNSSEC a try.&lt;/p&gt;

&lt;p&gt;I deployed OpenDNSSEC on my personal FreeBSD server. The configuration didn&amp;rsquo;t take more than 15 minutes thanks to the excellent official &lt;a href=&#34;https://wiki.opendnssec.org/display/DOCS/OpenDNSSEC+Documentation+Home&#34;&gt;documentation&lt;/a&gt;. Once, I started the OpenDNSSEC service, it generated both KSK and ZSK for my zone. I published &lt;a href=&#34;https://tools.ietf.org/rfc/rfc3658.txt&#34;&gt;DS record&lt;/a&gt; via my registrar&amp;rsquo;s web management portal. I am using RSASHA256 cipher for both KSK and ZSK. I wanted to use ECDSA, but my registrar doesn&amp;rsquo;t support it yet. I hope this will change in the near future. Any DNSSEC-related operations are now made automatically without any manual intervertion. Whenever I add new DNS records into my zone, I just call ods-signer to resign my zone. The zone resigning will be scheduled and the new records will be published alongside with their signatures. I love automatic things!&lt;/p&gt;

&lt;p&gt;My zone also contains &lt;a href=&#34;https://tools.ietf.org/html/rfc4255&#34;&gt;SSHFP&lt;/a&gt; records with my SSH server fingerprints and I am now able to finally make use of OpenSSH client&amp;rsquo;s VerifyHostKeyDNS feature. I tried out &lt;a href=&#34;https://tools.ietf.org/html/rfc4025&#34;&gt;IPSECKEY&lt;/a&gt; record. It worked with racoon. More on how to configure racoon to get IPSec peer&amp;rsquo;s certificate from the DNS can be found in racoon.conf(5). Lastly, I am very interested in &lt;a href=&#34;https://tools.ietf.org/html/rfc6698&#34;&gt;DANE&lt;/a&gt; and TLSA resource record. I think that this will be the biggest feature of DNSSEC in the upcoming years because DANE makes commercial CAs obsolete. Why should I pay for certificates, when I can generate one and just publish it in the DNS? This is very nice feature, but it&amp;rsquo;s dependent on a wider DNSSEC adoption, which isn&amp;rsquo;t happening massively. I hope this will change pretty soon.&lt;/p&gt;

&lt;p&gt;I wrote simple howto about running OpenDNSSEC with BIND on FreeBSD. I published it on my &lt;a href=&#34;http://wiki.xenol.eu/doku.php?id=freebsd_bind_opendnssec&#34;&gt;wiki&lt;/a&gt; and I hope it will help somebody. Feedback is welcome. I am also planning to package both SoftHSM and OpenDNSSEC for &lt;a href=&#34;http://www.openindiana.org&#34;&gt;OpenIndiana&lt;/a&gt;, once I am done with other tasks I am working on. However, I do not expect this to happen during this month. More realistic date is December.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Easy FreeBSD jail management with ZFS and pkgng</title>
      <link>https://blog.xenol.eu/2012/02/04/easy-freebsd-jail-management-with-zfs-and-pkgng/</link>
      <pubDate>Sat, 04 Feb 2012 00:00:00 +0000</pubDate>
      
      <guid>https://blog.xenol.eu/2012/02/04/easy-freebsd-jail-management-with-zfs-and-pkgng/</guid>
      <description>&lt;p&gt;I have been using FreeBSD jails ever since I started using FreeBSD on my servers. Jail can be described as a chroot on steroids with own users, process namespace and lately own virtualized network stack.&lt;/p&gt;

&lt;p&gt;I am using FreeBSD jails for mainly for securing and separating services. Each service runs in its own separated container on its own ZFS dataset. Each dataset is a ZFS clone from the snapshot of the template jail, which is adapted to have software I need in every jail installed (zsh, git, vim&amp;hellip;) and configured. Running FreeBSD jails on separate ZFS datasets is very flexible because we can setup different per-jail mount options, use compression and snapshot each jail separately. Software updates are fine for small amount of jails, while updating software in hundreds of jails can be a scaring experience. This is past with &lt;a href=&#34;https://github.com/pkgng/pkgng&#34;&gt;pkgng&lt;/a&gt;. pkgng is a new project, which brings new binary package manager. It is very easy to build packages from ports, create and serving package repository and updating.&lt;/p&gt;

&lt;p&gt;With this setup, I can deploy new jail under 2 minutes as all I have to do is a ZFS snapshot cloning and adding jail configuration into host /etc/rc.conf and thanks to ZFS cloning capabilties we are saving some disk space, too. I wrote a simple &lt;a href=&#34;http://wiki.xenol.eu/doku.php?id=zfs_pkgng_jail&#34;&gt;howto&lt;/a&gt; on my personal wiki. I have also setup package repository for my needs at &lt;a href=&#34;http://pkg.xenol.eu&#34;&gt;pkg.xenol.eu&lt;/a&gt;. Feel free to use it.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>